"AI 탐정 보조" 웹 앱 기능 명세서

1. 프로젝트 개요

프로젝트명: AI 탐정 보조 (가제)

목표: 오프라인 추리 게임 현장에서 플레이어가 촬영한 '단서 사진'을 AI로 분석하고, 기획자가 의도한 '해석'을 제공하는 모바일 웹 애플리케이션 개발.

핵심 기술: Next.js, Gemini AI (Vision API - 이미지 분류/매칭용)

MVP 정책:

카메라 우선 (capture="environment") 정책 채택.

AI 프롬프트에 '사건/용의자' 전체 맥락(Context)을 포함.

분석 내역은 localStorage를 통해 영구 저장.

'결정적 단서'에 매칭될 때만 '사전 정의된 답변'을 제공 (Rule-based).

2. 화면 및 기능 정의

1. 스플래시 스크린 (Entry Point)

기능: 앱 로딩 시, 브랜딩을 노출합니다.

요구사항:

"Powered by Mi:dm, KT's AI model" 문구를 화면 중앙에 표시합니다. (MVP 단계에서는 별도 로고 파일 없이 텍스트로만 처리)

3초 후 메인 화면으로 자동 전환되거나, 사용자가 화면을 탭(Tap)하면 메인 화면으로 전환됩니다.

2. 메인 화면 (탐정 대시보드)

기능: 게임의 허브 역할을 하며, 정보 접근 및 핵심 기능(단서 분석)을 수행합니다.

요구사항:

UI 페르소나: "AI 탐정 보조"의 캐릭터(예: 지적인 AI 비서)가 느껴지는 디자인을 적용합니다.

헤더: "AI 탐정 보조" 또는 "사건 파일" 타이틀.

상시 접근 버튼 (헤더 또는 상단):

[사건 개요] 버튼: 클릭 시 '3. 사건 개요 모달' 활성화.

[용의자 정보] 버튼: 클릭 시 '4. 용의자 정보 모달' 활성화.

핵심 CTA (Floating Action Button 권장):

[+] 또는 [카메라] 아이콘 버튼 (단서 분석).

컨텐츠 영역 (분석 내역):

'5. 단서 분석 기능'을 통해 분석된 결과(이미지 + AI 의견)가 시간순으로 누적되는 로그(Log) 영역.

초기 상태: "분석할 단서를 촬영해 주세요." 안내 문구 표시.

3. 사건 개요 모달

기능: 플레이어가 언제든 사건의 기본 정보를 다시 확인할 수 있습니다.

요구사항:

'메인 화면'의 [사건 개요] 버튼 클릭 시, 하단 시트(Bottom Sheet) 또는 모달 팝업으로 표시됩니다.

제공된 '사건 개요' 텍스트(사망 원인, 추정 시간 등)를 표시합니다.

닫기(X) 버튼 또는 외부 영역 클릭 시 닫힙니다.

4. 용의자 정보 모달

기능: 플레이어가 용의자 4인의 정보를 비교/확인합니다.

요구사항:

'메인 화면'의 [용의자 정보] 버튼 클릭 시, 모달 팝업으로 표시됩니다.

UI 구현: 용의자 A, B, C, D의 정보를 탭(Tabs) 또는 아코디언(Accordion) UI로 구현하여 쉽게 전환하며 볼 수 있도록 합니다.

내용은 제공된 용의자 4인의 프로필, 관계, 알리바이를 포함합니다.

5. 단서 분석 기능 (핵심 기능)

기능: 현장의 단서를 촬영하여 AI에게 분석을 요청합니다.

요구사항:

트리거: '메인 화면'의 [단서 분석] CTA 버튼 클릭.

카메라 실행: 브라우저의 네이티브 카메라 기능을 즉시 실행합니다.

<input type="file" accept="image/*" capture="environment"> 속성을 사용하여 '파일 업로드'가 아닌 '후면 카메라'를 즉시 활성화합니다.

이미지 전송: 사용자가 사진 촬영을 완료하면, 해당 이미지 파일을 백엔드 API(POST /api/analyze)로 전송합니다.

로딩 상태: API 응답 대기 중에는 '메인 화면' 컨텐츠 영역에 "단서를 분석 중입니다..."와 같은 로딩 인디케이터를 표시합니다.

결과 표시: AI의 분석 텍스트를 받으면, '메인 화면' 컨텐츠 영역(분석 내역)에 "채팅 버블" 형태로 결과를 추가합니다.

(단서 매칭 시 예시) [플레이어 촬영 이미지] + [AI 보조]: "왼쪽 소매에 물감이 묻어있군요. 왼손잡이일 가능성이 높습니다."

(단서 미매칭 시 예시) [플레이어 촬영 이미지] + [AI 보조]: "흠... 이건 결정적인 단서는 아닌 것 같네요. 좀 더 자세히 살펴보시죠."

내역 저장 (필수): 분석된 내역(이미지 URL, AI 답변)은 브라우저 localStorage에 JSON 형태로 저장하여, 앱을 새로고침해도 분석 기록이 유지되도록 합니다.

3. 백엔드 (Next.js API Route) 및 AI 연동 (Rule-based)

POST /api/analyze

역할: 프론트엔드에서 받은 이미지가 '결정적 단서'인지 Gemini AI를 통해 **분류(Classify)**하고, 매칭되는 '사전 정의된 답변'을 반환합니다.

Input: 이미지 파일 (Multipart/form-data)

Process:

이미지 처리: 수신된 이미지의 용량을 최적화(리사이징/압축)합니다.

사전 정의된 단서/답변 DB (예시):

CLUE_01 (왼쪽 소매): "작업복의 왼쪽 소매와 주머니에 물감이 묻어있는 것으로 볼때 화가는 왼손잡이로 추정됩니다"

CLUE_02 (오른손의 붓): "피해자는 오른손에 붓을 쥐고 있습니다. 누군가 사건 이후 현장을 조작했을 가능성이 있습니다"

CLUE_03 (터펜타인): "터펜타인은 유화물감 희석제입니다"

CLUE_NONE (기타): "흠... 이건 결정적인 단서는 아닌 것 같네요. 좀 더 자세히 살펴보시죠."

Gemini API 호출 (분류 목적):

이미지 데이터와 함께 **'분류용 시스템 프롬프트'**를 전송합니다.

(필수) 시스템 프롬프트 예시:

"당신은 천재적인 AI 탐정 보조입니다. 지금부터 가상의 살인 사건을 수사합니다.
[사건 개요: {사건 개요 텍스트}]
[용의자: {용의자 4인 정보 텍스트}]
[결정적 단서 리스트: 
 1. CLUE_01: 피해자의 왼쪽 소매에 묻은 물감
 2. CLUE_02: 피해자의 오른손에 쥐어진 붓
 3. CLUE_03: 터펜타인 유리병]

위 정보를 바탕으로, 사용자가 제공한 사진이 [결정적 단서 리스트] 중 무엇에 해당하는지 정확히 분류하세요.
사진이 리스트와 일치하지 않으면 'CLUE_NONE'이라고 답하세요.
오직 'CLUE_01', 'CLUE_02', 'CLUE_03', 'CLUE_NONE' 중 하나의 ID로만 대답해야 합니다."


답변 매칭: Gemini API로부터 CLUE_01과 같은 ID를 응답 받습니다.

최종 응답: '2. 사전 정의된 단서/답변 DB'에서 해당 ID의 텍스트(예: "작업복의 왼쪽 소매와...")를 조회하여 프론트엔드에 JSON 형태로 반환합니다.

Output: { "analysis": "사전 정의된 답변 텍스트" }

---

# 개발 구현 단계

## 📊 프로젝트 현황

### 핵심 기술 스택
- **프론트엔드**: Next.js
- **AI 연동**: Gemini AI (Vision API)
- **데이터 저장**: localStorage
- **타겟 플랫폼**: 모바일 웹 (카메라 활용)

---

## 🎯 단계별 구현 계획

### **Phase 1: 프로젝트 초기 설정 및 기본 구조**
1. Next.js 프로젝트 초기화 (TypeScript 권장)
2. 필요한 의존성 패키지 설치 (Gemini AI SDK 등)
3. 프로젝트 폴더 구조 설계
4. 환경변수 설정 (.env.local - Gemini API Key)
5. Tailwind CSS 또는 스타일링 라이브러리 설정

### **Phase 2: UI 컴포넌트 개발**
1. **스플래시 스크린** 구현
   - 3초 자동 전환 또는 탭으로 스킵 기능
   - "Powered by Mi:dm, KT's AI model" 브랜딩

2. **메인 화면 (탐정 대시보드)** 구현
   - 헤더 및 타이틀
   - [사건 개요], [용의자 정보] 버튼
   - Floating Action Button (단서 분석)
   - 분석 내역 로그 영역 (채팅 버블 UI)

3. **모달 컴포넌트** 구현
   - 사건 개요 모달 (Bottom Sheet)
   - 용의자 정보 모달 (Tabs/Accordion UI)

### **Phase 3: 카메라 및 이미지 처리 기능**
1. 카메라 컴포넌트 개발
   - `<input type="file" accept="image/*" capture="environment">` 구현
   - 이미지 미리보기 기능
   
2. 이미지 최적화 로직
   - 클라이언트 사이드 리사이징/압축
   - Base64 또는 FormData 변환

### **Phase 4: 백엔드 API 개발**
1. **POST /api/analyze 엔드포인트** 구현
   - 이미지 파일 수신
   - Gemini Vision API 연동
   - 단서 분류 시스템 프롬프트 작성
   - Rule-based 답변 매칭 로직

2. **단서 데이터베이스** 정의
   - CLUE_01, CLUE_02, CLUE_03, CLUE_NONE
   - 각 단서별 사전 정의된 답변 텍스트

### **Phase 5: 상태 관리 및 데이터 저장**
1. localStorage 연동
   - 분석 내역 저장/불러오기
   - JSON 직렬화/역직렬화
   
2. 상태 관리 (Context API 또는 Zustand)
   - 분석 내역 상태
   - 로딩 상태
   - 모달 상태

### **Phase 6: 게임 콘텐츠 작성**
1. 사건 개요 텍스트 작성
2. 용의자 4인 정보 작성
3. 결정적 단서 리스트 및 답변 작성
4. Gemini AI 시스템 프롬프트 최적화

### **Phase 7: UI/UX 개선 및 반응형 대응**
1. 모바일 최적화 (터치, 제스처)
2. 로딩 인디케이터 및 피드백
3. 에러 핸들링 및 사용자 안내
4. AI 탐정 보조 페르소나 디자인 적용

### **Phase 8: 테스트 및 최적화**
1. 카메라 기능 실제 디바이스 테스트
2. Gemini AI 분류 정확도 테스트
3. localStorage 용량 및 성능 테스트
4. 크로스 브라우저 테스트

### **Phase 9: 배포**
1. Vercel 또는 호스팅 플랫폼 설정
2. 환경변수 프로덕션 설정
3. PWA 설정 (선택사항)
4. 성능 모니터링 설정

---

## 💡 권장 개발 우선순위

**최소 기능 제품(MVP)**을 빠르게 구현하기 위한 권장 순서:

1. ✅ **Phase 1** - 프로젝트 설정 (완료)
2. ✅ **Phase 2** - UI 컴포넌트 (완료)
3. ✅ **Phase 3** - 카메라 및 이미지 처리 (완료)
4. ✅ **Phase 4** - Gemini AI 연동 (완료)
5. ⚡ **Phase 5** - 데이터 저장 (완료)
6. ⚡ **Phase 6** - 게임 콘텐츠 (완료)
7. ⚡ **Phase 7, 8, 9** - 개선 및 배포

---

## 🎉 현재 구현 상태

### ✅ 완료된 기능

#### **Phase 1-3: 기본 시스템** (100% 완료)
- ✅ Next.js + TypeScript + Tailwind CSS 설정
- ✅ KTFlow 폰트 적용
- ✅ 스플래시 스크린 (브랜딩 중심)
- ✅ 메인 대시보드 (모바일 최적화)
- ✅ 사건 개요 모달
- ✅ 용의자 정보 모달 (탭 UI)
- ✅ 이미지 미리보기 및 확인
- ✅ 이미지 자동 압축 (1024x1024, 85% 품질)
- ✅ localStorage 저장/불러오기

#### **Phase 4: Gemini AI 연동** (100% 완료)
- ✅ Gemini 1.5 Flash Vision API 통합
- ✅ 이미지 기반 단서 분류
- ✅ 동적 시스템 프롬프트 생성
- ✅ Rule-based 답변 시스템
- ✅ API 에러 핸들링
- ✅ API 키 검증 및 설정 가이드

### 📱 주요 기능

1. **AI 단서 분석**
   - 이미지 업로드 → Gemini AI 분석 → 단서 분류
   - 3가지 결정적 단서 (CLUE_01, 02, 03) 자동 인식
   - 맥락 기반 답변 제공

2. **모바일 최적화**
   - 터치 친화적 UI
   - Safe Area 지원 (노치, 홈 바)
   - 이미지 자동 압축 (데이터 절약)
   - 반응형 디자인

3. **게임 데이터**
   - 사건 개요: "화가의 죽음"
   - 용의자 4인 정보 (프로필, 알리바이)
   - 결정적 단서 3개

### 🔧 API 키 설정 필요

Gemini AI를 사용하려면 API 키 설정이 필요합니다:

👉 **자세한 가이드: [API_KEY_SETUP.md](./API_KEY_SETUP.md)**

**간단 요약:**
1. [Google AI Studio](https://aistudio.google.com/apikey)에서 API 키 발급
2. 프로젝트 루트에 `.env.local` 파일 생성
3. `GEMINI_API_KEY=여기에_API_키` 입력
4. 개발 서버 재시작

---

## 📁 예상 프로젝트 구조

```
investigator-assistant/
├── src/
│   ├── app/
│   │   ├── page.tsx                 # 메인 페이지
│   │   ├── splash/
│   │   │   └── page.tsx             # 스플래시 스크린
│   │   └── api/
│   │       └── analyze/
│   │           └── route.ts         # AI 분석 API
│   ├── components/
│   │   ├── ui/                      # 재사용 가능한 UI 컴포넌트
│   │   ├── CameraCapture.tsx        # 카메라 컴포넌트
│   │   ├── AnalysisLog.tsx          # 분석 내역 로그
│   │   ├── CaseOverviewModal.tsx    # 사건 개요 모달
│   │   └── SuspectInfoModal.tsx     # 용의자 정보 모달
│   ├── lib/
│   │   ├── gemini.ts                # Gemini AI 연동
│   │   ├── storage.ts               # localStorage 유틸
│   │   └── clues.ts                 # 단서 데이터베이스
│   ├── types/
│   │   └── index.ts                 # TypeScript 타입 정의
│   └── data/
│       ├── case-info.ts             # 사건 개요 데이터
│       └── suspects.ts              # 용의자 정보 데이터
├── public/
├── .env.local                       # 환경변수 (Gemini API Key)
├── package.json
└── README.md
```